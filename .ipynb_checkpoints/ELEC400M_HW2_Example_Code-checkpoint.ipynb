{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2838f0",
   "metadata": {},
   "source": [
    "## HW2 Unsupervised and Supervised Learning\n",
    "\n",
    "**Deadline** 11:59 pm on November 3rd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db104f36",
   "metadata": {},
   "source": [
    "In this assignment you'll gain some hands-on experience with principal components analysis (PCA) and Supervised Learning methods such as Support Vector Machine (SVM) and Random Forest.\n",
    "\n",
    "You need to install the following libraries: tensorflow and pillow. But if you use co-lab, no need for additional installations. \n",
    "\n",
    "In the first problem, you will study how different numbers of principal components represent the images visually. For the second problem you will utilize sklearn built in functions to perform classification on the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f66b19",
   "metadata": {},
   "source": [
    "### Problem 2.1: PCA for dimension reduction (3 Points)\n",
    "\n",
    "In this problem you will approximately reconstruct images by simplifying them to multiples of a few principal components.\n",
    "\n",
    "Note: When you display the images, use the color map `cmap=plt.cm.gray.reversed()` for MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b26cf",
   "metadata": {},
   "source": [
    "Pick a random seed in the next cell to select a random image of a handwritten $0$ from the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d4bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, y), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x = x.reshape([60000, 28*28])\n",
    "zeros = np.where(y==0)[0]\n",
    "x = x[zeros,:]\n",
    "y = y[zeros]\n",
    "np.random.seed(265) # put your seed here\n",
    "my_image = np.random.randint(0, len(y), size=1)\n",
    "\n",
    "plt.imshow(x[my_image,:].reshape((28,28)), cmap=plt.cm.gray.reversed())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42be90e",
   "metadata": {},
   "source": [
    "For $k = 0, 10, 20, 30, 40, 50$, use $k$-th principal components for MNIST $0$'s to approximately reconstruct the image selected above. Noting that we index from 0, namely 0-th pricipal component is the first one. Display the reconstruction for each value of $k$. To display the set of images compactly, you may want to use the 'plot_images' function defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902354a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, titles, h, w, n_row=3, n_col=4, reversed=False):\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        if reversed:\n",
    "            plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        else:\n",
    "            plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray.reversed())\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3064453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint example code\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "height = 28\n",
    "width = 28\n",
    "\n",
    "\n",
    "num_components = 50\n",
    "\n",
    "#initialize and fit PCA\n",
    "pca = PCA(num_components).fit(x)\n",
    "\n",
    "#get principal_vectors (components) of the fitted pca (size = (n_components, n_features)). The components are sorted by explained_variance_.\n",
    "principal_vectors = \n",
    "\n",
    "#reshape the principals vectors to the same size of input images\n",
    "principal_vectors = principal_vectors.reshape((num_components, height, width))\n",
    "\n",
    "# fit the model with x and apply the dimensionality reduction on x.\n",
    "pcs = \n",
    "\n",
    "# Transform data back to its original space\n",
    "capprox = \n",
    "\n",
    "# plot\n",
    "labels = ['principal vector %d' % (i+1) for i in np.arange(num_components)]\n",
    "\n",
    "plot_images( #fill in# )\n",
    "\n",
    "# show the obtained total variance\n",
    "ratio = pca.explained_variance_ratio_.sum()\n",
    "print('Variance explained by first %d principal vectors: %.2f%%' % (num_components, ratio*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812726f1",
   "metadata": {},
   "source": [
    "### Problem 2.2: SVM for classification (7 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import seaborn as sns # for statistical data visualization\n",
    "import sklearn.metrics as metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b203e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = pd.read_csv('./data/train.csv')\n",
    "test_data = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# Set variables for the targets and features\n",
    "y_train = train_data['price_range']\n",
    "X_train = train_data.drop('price_range', axis=1)\n",
    "y_test = test_data['price_range']\n",
    "X_test = test_data.drop('price_range', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df5ea78",
   "metadata": {},
   "source": [
    "### 2.2.a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183273fb",
   "metadata": {},
   "source": [
    "The linear kernel is written as $<x,x'>$.\n",
    "\n",
    "The parameter C, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ceb38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# TODO: Train Linear kernel SVM for different values of C on train data\n",
    "Cs = [   ] # TODO: fill in the hyper-parameter candidates\n",
    "for c in Cs:\n",
    "  lsvc = LinearSVC(random_state = 7, C = c)\n",
    "  # TODO: Fit the model and get prediction and evalutation on testing data.\n",
    "\n",
    "  # TODO: Save your results\n",
    "\n",
    "# TODO: Plot accuracy on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa87549e",
   "metadata": {},
   "source": [
    "### 2.2.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e826e8d9",
   "metadata": {},
   "source": [
    "The RBF kernel is expressed as $exp(Î³\\|x-x' \\|)$.\n",
    "\n",
    "$\\gamma$ defines how much influence a single training example has. The larger $\\gamma$ is, the closer other examples must be to be affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db17f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# TODO: Train RBF kernel SVM for different values of gamma on train data\n",
    "\n",
    "gammas = [   ] # TODO: fill in the hyper-parameter candidates\n",
    "for g in gammas:\n",
    "  rsvc = SVC(random_state = 7, C=1.0, kernel='rbf', gamma = g)\n",
    "  # TODO: Fit the model and get prediction and evalutation on testing data.\n",
    "\n",
    "  # TODO: Save your results\n",
    "\n",
    "\n",
    "# TODO: Plot accuracy on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ec5d7",
   "metadata": {},
   "source": [
    "### 2.2.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd88dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_trees = [] # TODO: fill in the hyper-parameter candidates\n",
    "# TODO: Train Random Forest for different values of number of estimators on train data\n",
    "for n in n_trees:\n",
    "  rf = RandomForestClassifier(random_state = 7, n_estimators=n)\n",
    "  # TODO: Fit the model and get prediction and evalutation on testing data.\n",
    "\n",
    "  # TODO: Save your results\n",
    "\n",
    "# TODO: Plot accuracy on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd8e227",
   "metadata": {},
   "source": [
    "### 2.2.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "# configure the cross-validation procedure\n",
    "cv = KFold(n_splits=n_folds, shuffle=True, random_state=1)\n",
    "\n",
    "\n",
    "# TODO: define search space\n",
    "space = dict()\n",
    "\n",
    "# TODO: define the model for parts a, b, and C\n",
    "lsvc = \n",
    "rsvc =\n",
    "rf = \n",
    "\n",
    "# TODO: Perform a grid search and cross-validation to find the optimal hyperparameters of parts a, b, and c\n",
    "\n",
    "# TODO: For each part, report the optimal value\n",
    "\n",
    "# TODO: For each part, report the accuracy on test data for the best estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5c5db",
   "metadata": {},
   "source": [
    "Hint: Here I provide an example code of how to use GridSearchCV. For more information, please refer to [Scikit Learn - GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a427f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is only a piece of example code of how to use GridSearchCV.\n",
    "You don't need to do anything here. \n",
    "You can refer to this example code and implement Prob 2.d.\n",
    "'''\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(X_train, y_train)\n",
    "\n",
    "# get the best performing model fit on the whole training set\n",
    "best_model = result.best_estimator_\n",
    "\n",
    "# evaluate model on the hold out dataset\n",
    "yhat = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257529dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
